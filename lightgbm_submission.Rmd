---
title: "lightGBM submission"
author: "Yiru Gong, yg2832"
date: "`r Sys.Date()`"
output: html_document
---

```{r, warning=FALSE}
library(data.table)
library(readxl)
library(tidyverse)
library(VIM)
library(dplyr)
library(recipes)
library(caret)
library(ranger)
library(pROC)
library(xgboost)
library(keras)
library(tfruns)
library(mice)
```

## data input

```{r}
# load("final_na_enlarged.Rdata")
# load("balanced_data.Rdata") ## for balanced train data: data_rose

load("final.Rdata") ## for original full dataset: final
cat_idx2 = c(cat_idx, 'metro')

# load("final.Rdata") ## for original full dataset: final
# cat_idx2 = c(cat_idx, 'metro')

# load("final_869_unimputed.Rdata")

## training data
# train = data_rose ##for balanced data
train = final[rowTrain,] ##for original data
x = train[,-1]  
y = train$hi_flag  

## testing data
test = final[-rowTrain,]
x2 = final[-rowTrain,-1]   
y2 = final$hi_flag[-rowTrain]

```

```{r}
library(lightgbm)
library(MLmetrics)

## Notice: hi_flag is at the first column of dataset !!
train_lgb = train %>%
  mutate(hi_flag = as.numeric(hi_flag)) %>%
  mutate_if(is.factor, as.numeric) %>%
  as.matrix()
test_lgb = test %>%
  mutate(hi_flag = as.numeric(hi_flag)) %>%
  mutate_if(is.factor, as.numeric) %>%
  as.matrix()

head(colnames(train_lgb))
lgb_train = lgb.Dataset(data=train_lgb[,-1],
                        label=train_lgb[,1],
                        categorical_feature = cat_idx2)

head(colnames(test_lgb))
x2_lgb = test_lgb[,-1]
y2_lgb = test_lgb[,1]
lgb_test = lgb.Dataset(data=x2_lgb,
                       label=y2_lgb,
                       categorical_feature = cat_idx2)
```

## Model training

```{r}
## default parameters
lgb.grid = list(objective = "binary",
                metric = "auc",
                max_depth = 2,
                min_sum_hessian_in_leaf = 1,
                feature_fraction = 0.036,
                bagging_fraction = 0.9,
                bagging_freq = 5,
                min_data = 100,
                max_bin = 50,
                lambda_l1 = 2.5711,
                lambda_l2 = 0.1927,
                min_data_in_bin=100,
                min_gain_to_split = 10,
                min_data_in_leaf = 30,
                # is_unbalance = TRUE,
                scale_pos_weight = 30)

# Evaluation: Gini for Lgb
lgb.normalizedgini = function(preds, dtrain){
  actual = lightgbm::getinfo(dtrain, "label")
  score  = NormalizedGini(preds,actual)
  return(list(name = "gini", value = score, higher_better = TRUE))
}
```

```{r}
start = Sys.time()
start

set.seed(1)
lgb.model.cv = lgb.cv(params = lgb.grid, data = lgb_train, num_leaves = 25,
                  num_threads = 4 , nrounds = 3000, early_stopping_rounds = 50,
                  eval_freq = 20, eval = lgb.normalizedgini,
                  categorical_feature = cat_idx2, nfold = 5, stratified = TRUE)

best.iter = lgb.model.cv$best_iter #1073
lgb.model.cv$best_score #0.7555599

Sys.time() - start
```

```{r}
set.seed(1)
lgb.model = lgb.train(params = lgb.grid, data = lgb_train, learning_rate = 0.01, 
                      num_leaves = 25,num_threads = 2 , nrounds = best.iter,
                      eval_freq = 20, eval = lgb.normalizedgini,
                      categorical_feature = cat_idx2)

lgb_pred = predict(lgb.model,x2_lgb)
roc(y2_lgb, lgb_pred)
```

## Grid Tuning

```{r}
grids_full = expand.grid(max_depth = c(1,2,3),
                # learning_rate = c(0.01,0.005),
                # min_sum_hessian_in_leaf = seq(0,1,0.1),  #seq(0.0005,0.01,0.002),
                feature_fraction = round(runif(10,min = 0, max = 0.3), 3),   #c(0.1,0.3,0.5,0.7,0.9),
                lambda_l1 = runif(10,min = 1.5, max = 6),
                lambda_l2 = runif(10,min = 0, max = 1),
                # min_gain_to_split = 10,  #seq(1,15,4),
                # min_data_in_leaf = c(30,100,500,1000,2000)
                # bagging_fraction = c(0.1,0.3,0.5,0.7,0.9)
                bagging_freq = c(5,10),
                # min_data = c(3,50,100, 500),
                # max_bin = 50,
                # min_data_in_bin=c(10, 50,100)
                scale_pos_weight = sample(c(25:75),10)
                )
# grids_full = grids_full %>% filter(min_data >= min_data_in_bin)
grids = grids_full[sample(1:nrow(grids_full), 0.005*nrow(grids_full)),]

start = Sys.time()
start

performance = data.frame(matrix(ncol = 3, nrow = 0))
colnames(performance) = c('comb','best_iter', "best_score")
for (i in c(1:nrow(grids))){
  # lgb_train = lgb.Dataset(data=train_lgb[,-1],
  #                       label=train_lgb[,1],
  #                       categorical_feature = cat_idx2,
  #                       min_data_in_bin=grids$min_data_in_bin[i])
  
  lgb.grid = list(objective = "binary",
                metric = "auc",
                max_depth = grids$max_depth[i],
                learning_rate = 0.01, #grids$learning_rate[i],
                min_sum_hessian_in_leaf = 1,
                feature_fraction = grids$feature_fraction[i],
                bagging_fraction =  0.9, #grids$bagging_fraction[i],
                bagging_freq = grids$bagging_freq[i],
                min_data = 100, #grids$min_data[i],
                max_bin = 50,
                lambda_l1 = grids$lambda_l1[i],
                lambda_l2 = grids$lambda_l2[i],
                min_data_in_bin= 100, #grids$min_data_in_bin[i],
                min_gain_to_split = 10, #grids$min_gain_to_split[i],
                min_data_in_leaf = 30,
                # is_unbalance = TRUE)
                scale_pos_weight = grids$scale_pos_weight[i])
  
  lgb.model.cv = lgb.cv(params = lgb.grid, data = lgb_train, num_leaves = 25,
                  num_threads = 4 , nrounds = 3000, early_stopping_rounds = 50,
                  eval_freq = 20, eval = lgb.normalizedgini,
                  categorical_feature = cat_idx2, nfold = 5, stratified = TRUE, data_seed = 1)
  record = data.frame(comb=i, 
                      best_iter=lgb.model.cv$best_iter, 
                      best_score=lgb.model.cv$best_score)
  print(record)
  performance = rbind(performance,record)
}

best = performance[which.max(performance$best_score),]
print(best)

p2 = cbind(performance, grids) %>%
  arrange(desc(best_score))

print(p2[1,])

Sys.time() - start

par(mfrow=c(3,3))
plot(best_score~., p2[,-c(1:2)],alpha=0.5)

## round 1 best
# comb best_iter best_score max_depth feature_fraction lambda_l1 lambda_l2
# 1   83      1199  0.7584805         2              0.2         4       0.5

## round 2 best
#   comb best_iter best_score max_depth feature_fraction lambda_l1 lambda_l2 bagging_fraction
# 1  172      2479  0.7535647         1              0.1       4.5       0.3              0.9
```

  comb best_iter best_score max_depth feature_fraction lambda_l1 lambda_l2
1   47      1964  0.7586735         2            0.036  2.571139 0.1927161
  bagging_freq scale_pos_weight
1            5               30

```{r}
# Train final model
# i = best$comb
# lgb_train = lgb.Dataset(data=train_lgb[,-1],
#                         label=train_lgb[,1],
#                         categorical_feature = cat_idx2,
#                         min_data_in_bin=p2$min_data_in_bin[1])
lgb.grid = list(objective = "binary",
                metric = "auc",
                max_depth = p2$max_depth[1],
                learning_rate = 0.01, #p2$learning_rate[1],
                min_sum_hessian_in_leaf = 1,
                feature_fraction = p2$feature_fraction[1],
                bagging_fraction = 0.9, # p2$bagging_fraction[1],
                bagging_freq = p2$bagging_freq[1],
                min_data = 100, # p2$min_data[1],
                max_bin = 50,
                lambda_l1 = p2$lambda_l1[1],
                lambda_l2 = p2$lambda_l2[1],
                min_data_in_bin= 100, #p2$min_data_in_bin[1],
                min_gain_to_split = 10, #grids$min_gain_to_split[i],
                min_data_in_leaf = 30,
                # is_unbalance = TRUE)
                scale_pos_weight = p2$scale_pos_weight[1])

lgb.model = lgb.train(params = lgb.grid, data = lgb_train, num_leaves = 25,
                      num_threads = 4 , nrounds = p2$best_iter[1],
                      eval_freq = 20, eval = lgb.normalizedgini,
                      categorical_feature = cat_idx2)

lgb_pred = predict(lgb.model,x2_lgb)
roc(y2_lgb, lgb_pred) #0.7394

save(p2, lgb.model, file= "tmp_best.Rdata")
```



## Result submission

```{r}
# holdout data manipulation
all_feature = colnames(select(final,-hi_flag))
cat_idx2 = c(cat_idx,"metro")
num_idx = setdiff(all_feature,cat_idx2)

## feature selection
holdout = read.csv("../2022_Competition_Holdout.csv") %>%
  janitor::clean_names()
holdout[holdout=='null'] = NA
holdout_id = holdout$id

holdout = holdout %>%
  mutate(metro = ifelse(grepl("Metro", rucc_category) , 1, 0),
         metro = as.factor(metro) ) %>%
  select_at(all_feature) %>%
  mutate_at(cat_idx2, as.factor) %>%
  mutate_at(num_idx,as.numeric)
### view the NA condition
h.na = sapply(holdout,function(x) sum(is.na(x)))
h.na[which(h.na>0)]
```

## lightgbm model training

## Ver1: tunned lightGBM
updated: to tunned lightGBM

```{r}
load("holdout_imp_final.Rdata")
final_idx = colnames(select(final, -hi_flag))
holdout = holdout_imp_final[,final_idx]
```

```{r}
## data input
train_lgb = final %>%
  mutate(hi_flag = as.numeric(hi_flag)) %>%
  mutate_if(is.factor, as.numeric) %>%
  as.matrix()
test_lgb = holdout %>%
  mutate_if(is.factor, as.numeric) %>%
  as.matrix()
colnames(train_lgb)

lgb_train = lgb.Dataset(data=train_lgb[,-1],
                        label=train_lgb[,1],
                        categorical_feature = cat_idx2)
```

10.5
comb best_iter best_score max_depth feature_fraction lambda_l1 lambda_l2
1   83      1199  0.7584805         2              0.2         4       0.5

10.10
  comb best_iter best_score max_depth feature_fraction lambda_l1 lambda_l2
1   47      1964  0.7586735         2            0.036  2.571139 0.1927161
  bagging_freq scale_pos_weight
1            5               30

```{r}
## parameter sets
lgb.grid = list(objective = "binary",
                metric = "auc",
                max_depth = 2,
                min_sum_hessian_in_leaf = 1,
                feature_fraction = 0.036,
                bagging_fraction = 0.9,
                bagging_freq = 5,
                min_data = 100,
                max_bin = 50,
                lambda_l1 = 2.5711,
                lambda_l2 = 0.1927,
                min_data_in_bin=100,
                min_gain_to_split = 10,
                min_data_in_leaf = 30,
                # is_unbalance = TRUE,
                scale_pos_weight = 30)

# Evaluation: Gini for Lgb
lgb.normalizedgini = function(preds, dtrain){
  actual = lightgbm::getinfo(dtrain, "label")
  score  = NormalizedGini(preds,actual)
  return(list(name = "gini", value = score, higher_better = TRUE))
}
```

```{r}
start = Sys.time()
start

set.seed(1)
lgb.model.cv = lgb.cv(params = lgb.grid, data = lgb_train, learning_rate = 0.01, 
                      num_leaves = 25,
                      num_threads = 4, nrounds = 7000, early_stopping_rounds = 50,
                      eval_freq = 20, eval = lgb.normalizedgini,
                      categorical_feature = cat_idx2, nfold = 5, stratified = TRUE)

best.iter = lgb.model.cv$best_iter
lgb.model.cv$best_score

Sys.time() - start
```

```{r}
set.seed(1)
lgb.model = lgb.train(params = lgb.grid, data = lgb_train, learning_rate = 0.01, 
                      num_leaves = 25,num_threads = 4 , nrounds = best.iter,
                      eval_freq = 20, eval = lgb.normalizedgini,
                      categorical_feature = cat_idx2)

lgb_pred = predict(lgb.model,test_lgb)
output = cbind(ID = holdout_id, 
               SCORE = lgb_pred,
               RANK = rank(-lgb_pred, ties.method = "last"))
write.csv(output,"2022CaseCompetition_Yiru_Gong_20221013.csv",row.names = F)
```

